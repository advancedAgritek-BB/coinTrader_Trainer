default_window_days: 7
features:
  bollinger_window: 20
  bollinger_std: 2.0
  momentum_period: 10
  adx_period: 14
  return_threshold: 0.01
swarm:
  num_agents: 100
  rounds: 10
regime_lgbm:
  objective: multiclass
  metric: multi_logloss
  num_class: 3
  learning_rate: 0.05
  num_boost_round: 200
  early_stopping_rounds: 20
  num_leaves: 63
  feature_fraction: 0.8
  bagging_fraction: 0.7
  bagging_freq: 5
  # GPU options
  # Set device to 'opencl' to enable GPU training
  device: opencl
  gpu_platform_id: 0
  gpu_device_id: 0
  # Windows/AMD requires OpenCL drivers such as the Adrenalin/ROCm packages
  gpu_use_opencl: true
  # Windows users should install the Adrenalin or ROCm OpenCL drivers
  # and may need to adjust gpu_platform_id/gpu_device_id as reported by `clinfo`
  max_bin: 255

federated_regime:
  objective: multiclass
  metric: multi_logloss
  num_class: 3
  learning_rate: 0.05
  num_boost_round: 200
  early_stopping_rounds: 20
  num_leaves: 63
  feature_fraction: 0.8
  bagging_fraction: 0.7
  bagging_freq: 5
  # GPU options
  # Set device_type to 'gpu' to enable GPU training
  device_type: gpu
  gpu_platform_id: 0
  gpu_device_id: 0
  # Federated training options
  max_bin: 255
  num_clients: 10  # number of clients participating
  num_rounds: 20   # aggregation iterations
  local_epochs: 5

optuna:
  n_trials: 100
  direction: minimize

backtest:
  cash: 10000
  commission: 0.001
  slippage: 0.005
  costs: 0.002

rl:
  # Reinforcement-learning selector settings
  total_timesteps: 100000
  learning_rate: 0.0003
  exploration: 0.01
